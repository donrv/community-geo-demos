{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watershed Indicators Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The US EPA publishes an important set of tracking indicators for every watershed in the continental US through a program called Environmental Atlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 240 indicators in this set, and approxiately 8,000 watersheds (at \"HUC12 level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is possible to explore this dataset using the EnviroAtlas online web mapping tool, that system only lets you explore a single variable at a time.  With 240 variables, that can be rather cumbersome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting data like this in MapD supports a different form of exploratory analysis, where all indicators are simultaneously and interactively available.  This allows you to create high-level syntheses of the information, as well as to explore the relationships between indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pymapd import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "\n",
    "# put stuff here\n",
    "base_path = '/home/mapdadmin/demo/wshed'\n",
    "\n",
    "# URLs for regional xcel files are located\n",
    "base_indicators_url = 'https://www.epa.gov/sites/production/files/2017-02/170209wsio_indicator_data_v2.0_epa_region{}.xlsx'\n",
    "\n",
    "# the format of the downloaded excel file names\n",
    "indicators_base = '170209wsio_indicator_data_v2.0_epa_region{}.xlsx'\n",
    "\n",
    "# name for our regional csv intermediate files\n",
    "csv_base_filename = 'wsio_indicators_region{}.csv'\n",
    "\n",
    "# output table\n",
    "table_name = 'Watershed_Indicators' #name of derised output table in mapd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data (sequentially, since only a few, but could also use xargs to get in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for region in range(1,10):\n",
    "    zero_padded_region = '{:02d}'.format(region)\n",
    "    specific_url = base_indicators_url.format(zero_padded_region)\n",
    "    output_xls = indicators_base.format(zero_padded_region)\n",
    "    if not os.path.exists(output_xls):\n",
    "        print(\"Getting {}\".format(specific_url))\n",
    "        !wget {specific_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data are burried within a specific Excel sheet, we need to read each spreadsheet into memory, grabbing only the correct sheet, and then save back out to CSV.  We could for speed upload directly to MapD from a dataframe, using pymapd.  However in this case we prefer to keep an explicit copy on disk for public sharing in an open format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "for region in range(1,2):\n",
    "    zero_padded_region = '{:02d}'.format(region)\n",
    "    specific_file = indicators_base.format(zero_padded_region)\n",
    "    print(\"Processing {}\".format(specific_file))\n",
    "\n",
    "    of = csv_base_filename.format(zero_padded_region)\n",
    "    df = pd.read_excel(specific_file, sheet_name='WSIO_REGIONAL_DATA_TABLE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_col_names = df.columns\n",
    "sql_legal_col_names = []\n",
    "for cn in orig_col_names:\n",
    "    print(cn)\n",
    "    sql_legal_col_name = cn.title().replace(' ','').replace('%','Pcnt')\n",
    "    sql_legal_col_name = \"\".join(ch for ch in sql_legal_col_name if ch.isalnum())\n",
    "    #cn.title().replace(' ','').replace('(','').replace(')','').replace('%','Pcnt').replace(',','-')\n",
    "    print(\"   {}\".format(sql_legal_col_name))\n",
    "    sql_legal_col_names.append(sql_legal_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_offset = 0\n",
    "for cn in orig_col_names:\n",
    "    newname = sql_legal_col_names[name_offset]\n",
    "    print('Renaming col \\\"{}\\\" to \\\"{}\\\"'.format(cn, newname))\n",
    "    name_offset += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "for region in range(1,10):\n",
    "    zero_padded_region = '{:02d}'.format(region)\n",
    "    specific_file = indicators_base.format(zero_padded_region)\n",
    "    print(\"Processing {}\".format(specific_file))\n",
    "\n",
    "    of = csv_base_filename.format(zero_padded_region)\n",
    "    if not os.path.exists(of):\n",
    "        df = pd.read_excel(specific_file, sheet_name='WSIO_REGIONAL_DATA_TABLE')\n",
    "        df.columns = sql_legal_col_names  # force our new column names\n",
    "\n",
    "        # later, after debugging, add compression=gzip\n",
    "        df.to_csv(of, index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do: Figure out method for prettier column name truncation meeting MapD specs.  Current EPA column names are very long, and include commas.  MapD robustly converts and truncates them - but results can be hard to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to mapd default database\n",
    "con = connect(user=\"mapd\", password= \"HyperInteractive\", host='localhost', dbname=\"mapd\")\n",
    "con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wsio_indicators_create.sql', 'r') as myfile:\n",
    "    wsio_indicators_create=myfile.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    con.execute(wsio_indicators_create)\n",
    "except:\n",
    "    print('Problem creating database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for region in range(1,10):\n",
    "    zero_padded_region = '{:02d}'.format(region)\n",
    "    csv_file = csv_base_filename.format(zero_padded_region)\n",
    "    append_table_command = \"COPY {} FROM '{}'\".format(table_name, os.path.join(base_path,csv_file))\n",
    "    try:\n",
    "        print('Executing MapD command: {}'.format(append_table_command))\n",
    "        con.execute(append_table_command)\n",
    "    except:\n",
    "        print('Problem appending to master indicators table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /home/mapdadmin/demo/wshed/wsio_indicators_region01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail /var/lib/mapd/data/mapd_log/mapd_server.ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Watershed Boundary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wshed_shape = 'ftp://newftp.epa.gov/epadatacommons/ORD/EnviroAtlas/NHDPlusV2_WBDSnapshot_EnviroAtlas_CONUS.gdb.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget {wshed_shape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wshed_gdb_zip = 'NHDPlusV2_WBDSnapshot_EnviroAtlas_CONUS.gdb.zip'\n",
    "gdal_input = '/vsizip/{}'.format(wshed_gdb_zip) \n",
    "import sys,os,os.path\n",
    "os.environ['CPL_ZIP_ENCODING']='UTF-8'\n",
    "!gdalwarp -t_srs 'EPSG:4326' {gdal_input} 'us_watersheds_huc12.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "wshed = gpd.read_file(wshed_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wshed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip {wshed_gdb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,os.path\n",
    "os.environ['CPL_ZIP_ENCODING']='UTF-8'\n",
    "wshed_gdb = 'NHDPlusV2_WBDSnapshot_EnviroAtlas_CONUS.gdb'\n",
    "# -csql statement  \n",
    "# HUC_12: String (12.0)\n",
    "sql = 'SELECT *, CAST(HUC_12 AS INTEGER) FROM NHDPlusV2_WBDSnapshot_EnviroAtlas_CONUS'\n",
    "command = ogr2ogr -t_srs 'EPSG:4326' -sql {sql} {wshed_gdb} 'us_watersheds_huc12.shp'\n",
    "!ogr2ogr -t_srs 'EPSG:4326' -csql {sql} {wshed_gdb} 'us_watersheds_huc12.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ogr2ogr --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ogr2ogr -t_srs 'EPSG:4326' -sql 'SELECT Pct_Land, CAST(HUC_12 AS INTEGER) FROM NHDPlusV2_WBDSnapshot_EnviroAtlas_CONUS' 'us_watersheds_huc12.shp' NHDPlusV2_WBDSnapshot_EnviroAtlas_CONUS.gdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should probably drop Shape_Area column because its overflowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select us_watersheds_huc12.*,watershed_indicators_epa_region3.* from us_watersheds_huc12 INNER JOIN watershed_indicators_epa_region3 ON us_watersheds_huc12.HUC_122 = watershed_indicators_epa_region3.HydrologicUnitCode12DigitHUC12 ; "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
